{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## {{cookiecutter.project_name}}\n",
    "\n",
    "{{cookiecutter.description}}\n",
    "\n",
    "### Data Sources\n",
    "- file1 : Description of where this file came from\n",
    "\n",
    "### Changes\n",
    "- {% now 'utc', '%m-%d-%Y' %} : Started project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import datetime as dt\n",
    "import pickle\n",
    "import os\n",
    "from pandas import ExcelWriter\n",
    "import re\n",
    "from zipfile import ZipFile\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_columns(df, numunique= 205):\n",
    "    '''clean names of columns and caegorize object columns if unique is less than , numunique\n",
    "       downcast for int and floats'''\n",
    "    # https://stackoverflow.com/questions/30763351/removing-space-in-dataframe-python\n",
    "    df.columns = [x.strip() for x in df.columns]\n",
    "    df.rename(columns = lambda x: x.replace(\" \",\"_\"), inplace= True)\n",
    "    df.rename(columns = lambda x: x.replace(\".\",\"_\"), inplace= True)\n",
    "    df.rename(columns = lambda x: x.replace(\"&\",\"and\"), inplace= True)\n",
    "    df.rename(columns = lambda x: x.replace(\"(\",\"\"), inplace= True)\n",
    "    df.rename(columns = lambda x: x.replace(\")\",\"\"), inplace= True)\n",
    "    df.rename(columns = lambda x: x.replace(\"/\",\"\"), inplace= True)\n",
    "    df.rename(columns = lambda x: x.replace(\"-\",\"_\"), inplace= True)\n",
    "    df.rename(columns = lambda x: x.replace(\"+\",\"_plus_\"), inplace= True)\n",
    "    for y in df.columns:\n",
    "        #categorize columns\n",
    "        if df[y].dtype == np.object:\n",
    "            if len(df[y].unique()) <= numunique:\n",
    "                print('converted ' + y + ' ' +str(df[y].dtype) + ' records=' + str(len(df[y].unique())))\n",
    "                df[y] = df[y].astype('category')\n",
    "        elif(df[y].dtype == np.float64 or df[y].dtype == np.int64):\n",
    "            df[y] = pd.to_numeric(df[y], downcast='unsigned')\n",
    "            print('DOWNCAST ' +  y + ' ' +str(df[y].dtype))\n",
    "    return df\n",
    "\n",
    "def add_business_days(from_date, ndays):\n",
    "    '''Consider weekends when add days to a date'''\n",
    "    business_days_to_add = abs(ndays)\n",
    "    current_date = from_date\n",
    "    sign = ndays/abs(ndays)\n",
    "    while business_days_to_add > 0:\n",
    "        current_date += datetime.timedelta(sign * 1)\n",
    "        weekday = current_date.weekday()\n",
    "        if weekday >= 5: # sunday = 6\n",
    "            continue\n",
    "        business_days_to_add -= 1\n",
    "    return current_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileticketsname = input('Tickets file name: ')\n",
    "fileappsname = input('Apps file name: ')\n",
    "print(os.getcwd())\n",
    "originalpath = (os.getcwd())\n",
    "print(originalpath)\n",
    "\n",
    "os.chdir(originalpath)\n",
    "#os.chdir('..')\n",
    "path = os.getcwd()\n",
    "print(path)\n",
    "\n",
    "#filename = os.path.join(path, 'data','Power - Tickets - 2018 - v2.1-2.xlsx')\n",
    "filename = os.path.join(path, 'data','raw',fileticketsname)\n",
    "fileapps = os.path.join(path, 'data', 'raw', fileappsname)\n",
    "\n",
    "#outputfiles\n",
    "today = datetime.datetime.today()\n",
    "directory_name = '{{cookiecutter.directory_name}}'\n",
    "fileoriginaltickets = os.path.join(path, 'data','processed',\n",
    "                                   directory_name + '_tickets{:%m%d%y}.pkl').format(today)\n",
    "fileoriginalapps = os.path.join(path, 'data','processed',\n",
    "                                directory_name + '_apps{:%m%d%y}.pkl').format(today)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read files from Excel or CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if it is a CSV file ->tickets_file = pd.read_csv(filename)\n",
    "tickets_file = pd.ExcelFile(filename, \n",
    "                            converters= {'opened_at': pd.to_datetime, 'closed_at': pd.to_datetime})\n",
    "print(tickets_file.sheet_names)\n",
    "#get list of countries ina dataframe\n",
    "\n",
    "#read applications file\n",
    "apps_file = pd.ExcelFile(fileapps)\n",
    "print(apps_file.sheet_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheetname= input('Sheet to load for tickets: ')\n",
    "tickets_final = tickets_file.parse(sheetname, skiprows=0)\n",
    "print(len(tickets_final))\n",
    "\n",
    "sheetname= input('Sheet to load foro apps: ')\n",
    "apps_final = apps_file.parse(sheetname, skiprows=0)\n",
    "print(len(apps_final))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Cleanup\n",
    "\n",
    "- Remove all leading and trailing spaces\n",
    "- Get columns names and determine columns to rename df.columns\n",
    "- Rename the columns for consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean name columns, categorize columns, downcast columns\n",
    "tickets_final= categorize_columns(tickets_final)\n",
    "apps_final= categorize_columns(apps_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_rename = {'col1': 'New_Name'}\n",
    "tickets_final.rename(columns=cols_to_rename, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets_final.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For tickets file\n",
    "tickets_final.drop_duplicates(['number'], inplace= True)\n",
    "print(len(tickets_final))\n",
    "tickets_final.dropna(subset=['App_Bucket'], inplace= True)\n",
    "print(len(tickets_final))\n",
    "\n",
    "#add fields with correct hrs in EST\n",
    "tickets_final['x_opened_at_est'] = tickets_final['opened_at'] -  pd.to_timedelta(5, unit='h')\n",
    "tickets_final['x_closed_at_est'] = tickets_final['closed_at'] -  pd.to_timedelta(5, unit='h')\n",
    "\n",
    "tickets_final['x_yearmonth']= tickets_final['x_opened_at_est'].dt.strftime('%Y%m')\n",
    "tickets_final['x_dayweek']= tickets_final['x_opened_at_est'].dt.strftime('%w')\n",
    "\n",
    "date = pd.to_datetime(tickets_final['x_opened_at_est'])\n",
    "tickets_final['x_hourday']= date.dt.hour\n",
    "#tickets_final['hourday']= tickets_final['opened_at_est'].dt.strftime('%H')\n",
    "tickets_final['x_year']= tickets_final['x_opened_at_est'].dt.strftime('%Y')\n",
    "\n",
    "#set distribution\n",
    "'''adjust hourday to EST timezone and separate in 3 bins\n",
    "0,1  ,2,3,4,5,6,7,  8,9,10,11,12,13,14,15,16,17,18,19,  20,21,22,23,0, 1,   \n",
    "     '2-7'          '8-20'                              '20-2'           \n",
    "'''\n",
    "tickets_final['x_bins_day']= pd.cut(tickets_final['x_hourday'], bins=[-float(\"inf\"),1,7,19], \n",
    "                               labels=['20-2','2-8','8-20'])\n",
    "tickets_final['x_bins_day']= tickets_final['x_bins_day'].fillna('20-2')\n",
    "\n",
    "#cycletime\n",
    "tickets_final['x_cycletime'] = tickets_final['x_closed_at_est'] - tickets_final['x_opened_at_est']\n",
    "tickets_final['x_cycletime']=tickets_final['x_cycletime']/np.timedelta64(1,'D')\n",
    "tickets_final['x_cycletime'] = tickets_final['x_cycletime'].fillna(value=0, downcast='infer')\n",
    "\n",
    "#clean SSO where SSO = admin update to NaN\n",
    "#tickets_final['SSO_closed_by_name'] = tickets_final['closed_by_name'].str.extract('(\\d\\d\\d\\d\\d\\d\\d\\d\\d)', expand=True)\n",
    "tickets_final['x_closed_by_user_name']= tickets_final['closed_by_user_name']\n",
    "#tickets_final.loc[tickets_final['x_closed_by_user_name'] == 'admin' ,  'x_closed_by_user_name'] = np.nan\n",
    "#tickets_final['x_closed_by_user_name'] = pd.to_numeric(tickets_final['x_closed_by_user_name'], downcast=None, errors= 'coerce')\n",
    "\n",
    "tickets_final['x_Vendor_Closeassign'] = tickets_final['Vendor_Closeassign'].str.lower()\n",
    "\n",
    "#column to identify access management\n",
    "tickets_final['x_access_management'] = 0\n",
    "\n",
    "#add calculate SLA , due_date and In_SLA\n",
    "days_sla= {\n",
    "    '4 - Low' : 5,\n",
    "    '1 - Critical': .5,\n",
    "    '2 - High': 1,\n",
    "    '3 - Moderate' : 5 \n",
    "}\n",
    "tickets_final['priority'] = np.where(tickets_final['priority'] == '3 - Low', '4 - Low', tickets_final['priority'])\n",
    "tickets_final['x_days_sla']= tickets_final['priority'].map(days_sla).fillna(5)\n",
    "\n",
    "tickets_final['x_days_sla'].dtype == np.float64\n",
    "#add 5 days to days_sla if it is Incident\n",
    "tickets_final['x_days_sla']= np.where(tickets_final['Type']== 'Incident',\n",
    "                                      tickets_final['x_days_sla'] + 5,tickets_final['x_days_sla'])\n",
    "\n",
    "tickets_final['x_due_date']= tickets_final['x_opened_at_est'] + tickets_final['x_days_sla'].astype(\"timedelta64[D]\")\n",
    "tickets_final['x_inSLA'] = np.where(tickets_final['x_closed_at_est'] > tickets_final['x_due_date'], 0, \n",
    "                               np.where(tickets_final['x_closed_at_est'].isnull(), 0, 1))\n",
    "\n",
    "#caclualte aging based in extract date\n",
    "tickets_final['x_aging'] = np.where(tickets_final['state'].isin(['Pending', 'Work in Progress','Open', 'In Progress', 'On Hold', 'Submitted', 'Scheduled', 'Accepted']),\n",
    "                                  1,0) \n",
    "tickets_final['x_agingdays']= np.where(tickets_final['x_aging'] == 1,\n",
    "                                     dt.datetime(2019, 1, 31) - tickets_final['x_opened_at_est'],0)\n",
    "tickets_final['x_agingdays']=tickets_final['x_agingdays']/np.timedelta64(1,'D')\n",
    "\n",
    "tickets_final['x_vendor'] = np.where(tickets_final['x_Vendor_Closeassign'] == 'employee', 'GE',\n",
    "                                   np.where(tickets_final['x_Vendor_Closeassign'] == 'softtek','Softtek','Other'))\n",
    "\n",
    "#cosinder weekends in the duedate and verify if it is in SLA or OUTSLA\n",
    "tickets_final['x_due_date2'] = tickets_final.apply(lambda row: add_business_days(row['x_opened_at_est'],row['x_days_sla']), axis=1)\n",
    "tickets_final['x_inSLA2'] = np.where(tickets_final['x_closed_at_est'] > tickets_final['x_due_date2'], 0, \n",
    "                               np.where(tickets_final['x_closed_at_est'].isnull(), 0, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For apps file\n",
    "#fillna for columns that make sense to fillna\n",
    "#apps_final['L2_SSO'] = apps_final['L2_SSO'].fillna(value=0, downcast='infer')\n",
    "\n",
    "#add total of tickets for each app\n",
    "groupedappsbyticket= tickets_final.groupby(['cmdb_ci_name'], \n",
    "                            as_index = False).agg({'number' : 'count'})\n",
    "apps_final = pd.merge(apps_final,\n",
    "                     groupedappsbyticket[['cmdb_ci_name', 'number']] ,\n",
    "                     left_on='Application' ,\n",
    "                     right_on='cmdb_ci_name', \n",
    "                     how=\"left\", \n",
    "                     suffixes=('','x_ticket'))\n",
    "apps_final.rename(columns = {'number': 'x_tot_tickets'}, inplace= True)\n",
    "apps_final= apps_final.drop('cmdb_ci_name', axis=1)\n",
    "apps_final = apps_final.fillna({'x_tot_tickets': 0})\n",
    "\n",
    "#idetify type of service PbtD or SLA and vendor supported\n",
    "apps_final['x_supported_by']= 'Other'\n",
    "apps_final['x_service_type']= 'SLA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save output file into processed directory\n",
    "Save a file in the processed directory that is cleaned properly. It will be read in and used later for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save file a pkl for fast access\n",
    "tickets_final.to_pickle(fileoriginaltickets, 'gzip')\n",
    "print('original tickets file saved as {}'.format(fileoriginaltickets))\n",
    "\n",
    "#save file a pkl for fast access\n",
    "apps_final.to_pickle(fileoriginalapps, 'gzip')\n",
    "print('original apps file saved as {}'.format(fileoriginalapps))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
