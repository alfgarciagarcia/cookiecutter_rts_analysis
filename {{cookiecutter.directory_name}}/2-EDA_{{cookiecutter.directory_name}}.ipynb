{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## {{cookiecutter.project_name}}\n",
    "\n",
    "{{cookiecutter.description}}\n",
    "\n",
    "This notebook contains basic statistical analysis and visualization of the data.\n",
    "\n",
    "### Data Sources\n",
    "- summary : Processed file from notebook 1-Data_Prep\n",
    "\n",
    "### Changes\n",
    "- {% now 'utc', '%m-%d-%Y' %} : Started project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import datetime as dt\n",
    "import pickle\n",
    "import os\n",
    "from pandas import ExcelWriter\n",
    "import re\n",
    "from zipfile import ZipFile\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#80/20 analysis\n",
    "def pareto(df, rows, columns, sortcol, colmonth1, colmonth2, colmonth3, minvalue):\n",
    "    '''df= dataframe to use\n",
    "    rows= column to use for rows\n",
    "    columns= name of column to use as rows\n",
    "    sortcol = column to sumarrize ej: tickets, cases, persons, etc)\n",
    "    colmonth1:colmonth3 = columns to calculate average for columns\n",
    "    minvalue = value to filter the result, will show recrods with values greater than minvalue\n",
    "    '''\n",
    "    crostab= pd.crosstab(df[rows],df[columns],margins=True)\n",
    "    crostab.sort_values(sortcol, ascending=False,inplace=True)\n",
    "    crostab= crostab.drop(['All'])\n",
    "    print('Total of rows: {}'.format(len(crostab)))\n",
    "    crostab['pc']= 100*crostab[sortcol]/crostab[sortcol].sum()\n",
    "    crostab['cum_pc']=crostab['pc'].cumsum()\n",
    "    crostab['AVG3M']= (crostab[colmonth1] + crostab[colmonth2] + crostab[colmonth3]) /3\n",
    "    print('Total of rows up to 80%: {}'.format(len(crostab[crostab['cum_pc'] <  81])))\n",
    "    print('{} Total of rows below average of {}'.format(len(crostab[crostab['AVG3M'] <=  minvalue]), minvalue))\n",
    "    print('to print the table run: crostab2[crostab2[\"AVG3M\"] > 5]')\n",
    "    return crostab\n",
    "\n",
    "#distribution\n",
    "def gethrdistribution(df, group1, agg1, titletxt= 'Pie Chart', minpercent=5, filename='figpie.png'):\n",
    "    '''pie distributions per group\n",
    "    consolidate % < 10% in others category    \n",
    "    '''\n",
    "    dist1= df.groupby(group1,as_index=False)[agg1].count()\n",
    "    dist1['pc']= 100*dist1[agg1]/dist1[agg1].sum()\n",
    "    dist1[group1]= np.where(dist1['pc']<minpercent,'Others',dist1[group1])\n",
    "    dist1= dist1.groupby(group1,as_index=False)[agg1].sum()\n",
    "    dist1['pc']= 100*dist1[agg1]/dist1[agg1].sum()\n",
    "    dist1= dist1.sort_values('pc', ascending=False)\n",
    "    dist1.reindex(copy=False)\n",
    "    dist1['cum_pc']=dist1['pc'].cumsum()\n",
    "    # Create a list of colors (from iWantHue)\n",
    "    colors = [ '#959a3c', '#55ac69', '#5b86d0', \"#E13F29\", \"#D69A80\", \"#D63B59\", \n",
    "              \"#AE5552\", \"#CB5C3B\", \"#EB8076\", \"#96624E\" ]\n",
    "    # Create a pie chart\n",
    "    fig, ax = plt.subplots()  \n",
    "    plt.pie(\n",
    "        dist1[agg1],         # using data agg1\n",
    "        labels=dist1[group1],# with the labels being group1\n",
    "        shadow=False, # with no shadows\n",
    "        colors=colors, # with colors\n",
    "        #explode=(0, 0.15, 0), # with one slide exploded out\n",
    "        # with the start angle at 90%\n",
    "        startangle=90, # with the start angle at 90%\n",
    "        autopct='%1.1f%%', # with the percent listed as a fraction \n",
    "        counterclock= False\n",
    "        )\n",
    "    # View the plot drop above\n",
    "    plt.axis('equal')\n",
    "    plt.title(titletxt)\n",
    "    # View the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    figname_file= os.path.join(figures_path,\n",
    "                               directory_name + '_' + filename + '{:%m%d%y}.png').format(today)\n",
    "    fig.savefig(figname_file, transparent= True)\n",
    "    dist1= dist1.sort_values('pc', ascending=False)\n",
    "    print(dist1)\n",
    "\n",
    "def plottickets(df, group1, group2, countfield):\n",
    "    '''plot df grouped by group1 and group2 and counting countfield'''\n",
    "    ts=df.groupby([group1,group2]).agg({countfield: 'count'})\n",
    "    #ts.sort_values(group1, ascending=True,inplace=True)\n",
    "    ts.plot(kind= 'line')\n",
    "    return ts\n",
    "\n",
    "def weedaysbars(df, group1, agg1, title, xlab, ylab, filename='figbarcharth.png'):\n",
    "    '''function to display bar chart, ej criticality, or weekdays barcharts'''\n",
    "    weekdays= df.groupby(group1,as_index=False)[agg1].count()\n",
    "    fig, ax = plt.subplots()  \n",
    "    #plt.bar(weekdays[group1], height= weekdays[agg1], color='#607c8e')\n",
    "    ax.bar(weekdays[group1], height= weekdays[agg1], color='#607c8e')\n",
    "    \n",
    "    width = 0.75 # the width of the bars \n",
    "    ax.barh(df.index, df['number'], width)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    ###\n",
    "    #for i, v in enumerate(weekdays[group1]):\n",
    "    #    ax.text(v + 3, i + .0, str(v))\n",
    "    ax.patch.set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_linewidth(0.5)\n",
    "    ax.spines['left'].set_linewidth(0.5)\n",
    "    ###\n",
    "    plt.show()\n",
    "    figname_file= os.path.join(figures_path, \n",
    "                               directory_name + '_' + filename + '{:%m%d%y}.png').format(today)\n",
    "    fig.savefig(figname_file, transparent= True)\n",
    "    print(weekdays)\n",
    "    \n",
    "def weedaysbarsh(df, group1, agg1, title, xlab, ylab, filename='figbarcharth.png'):\n",
    "    '''function to display bar chart, ej criticality, or weekdays barcharts'''\n",
    "    weekdays= df.groupby(group1,as_index=False)[agg1].count()\n",
    "    fig, ax = plt.subplots()  \n",
    "    \n",
    "    width = 0.75 # the width of the bars \n",
    "    ax.barh(weekdays[group1], weekdays[agg1], width)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    for i, v in enumerate(weekdays[agg1]):\n",
    "        ax.text(v + 3, i + .0, str(v))\n",
    "    ax.patch.set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_linewidth(0.5)\n",
    "    ax.spines['left'].set_linewidth(0.5)\n",
    "    plt.show()\n",
    "    figname_file= os.path.join(figures_path, \n",
    "                               directory_name + '_' + filename + '{:%m%d%y}.png').format(today)\n",
    "    fig.savefig(figname_file, transparent= True)\n",
    "    print(weekdays)   \n",
    "    \n",
    "#cycle_time3\n",
    "def cycletime3(df, groupby2, groupby3, agg1, agg2, agg3):\n",
    "    '''Caclulate cycletime per vendor just for request and incidents\n",
    "    usage: cycletime2(nuclear0,'yearmonth','Vendor_Closeassign','cycletime','number',\n",
    "                    'closed_by_user_name', 'cycletime')\n",
    "    '''\n",
    "    df = df[df.Type.isin(['Requested Item','Incident'])]\n",
    "    #cycle_time and FTE\n",
    "    df2= df.groupby([groupby2,groupby3]).agg({agg1: ['mean','std','max','min'], \n",
    "                                                 agg2: 'count',agg3: 'nunique'})\n",
    "    # Using ravel, and a string join, we can create better names for the columns:\n",
    "    df2.columns = [\"_\".join(x) for x in df2.columns.ravel()]\n",
    "    agg5= agg3 + '_nunique'\n",
    "    agg6= agg2 + '_count'\n",
    "    agg7= agg1 + '_mean'\n",
    "    # per month\n",
    "    df2= df2.groupby([groupby3]).agg({agg5: ['mean', 'std'], agg6: ['mean','count', 'median','max'], \n",
    "                                      agg7: ['mean','std', 'median']})\n",
    "    return df2\n",
    "\n",
    "def barchart(df,x,y,title, x_label, y_label,filename='figbarchart.png'):\n",
    "    '''bar chart tickets per organizatio x_Vendor_Closeassign or vendor'''\n",
    "    field_vendor = 'x_Vendor_Closeassign'\n",
    "    field_vendor = 'x_vendor'\n",
    "    pt_df= df.pivot_table(x, index=[y],\n",
    "                                    aggfunc='count',\n",
    "                                    margins=True)#.sort_values(('SSO','All'), ascending=False)\n",
    "    pt_df.index.rename(y_label, inplace= True)\n",
    "    #remove rows with cero count of tickets\n",
    "    pt_df= pt_df[pt_df[x] >0].sort_values(x, ascending=True)\n",
    "\n",
    "    fig, ax = plt.subplots()    \n",
    "    width = 0.75 # the width of the bars \n",
    "    ax.barh(pt_df.index, pt_df[x], width)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)   \n",
    "    for i, v in enumerate(pt_df[x]):\n",
    "        ax.text(v + 3, i + .0, str(v))\n",
    "    ax.patch.set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_linewidth(0.5)\n",
    "    ax.spines['left'].set_linewidth(0.5)\n",
    "    plt.show()\n",
    "    figname_file= os.path.join(figures_path, \n",
    "                               directory_name + '_' + filename + '{:%m%d%y}.png').format(today)\n",
    "    fig.savefig(figname_file, transparent= True)\n",
    "\n",
    "def histogram(df,x, title, x_label, y_label, filter_in, filename= 'histogram'):\n",
    "    #histogram aging tickets \n",
    "    df_agging=df[(df.x_agingdays > 0) &(df.Type.isin(filter_in))]\n",
    "    df_agging= df_agging[x]\n",
    "    fig, ax = plt.subplots() \n",
    "    ax.hist(df_agging, bins=10)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label) \n",
    "    ax.patch.set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_linewidth(0.5)\n",
    "    ax.spines['left'].set_linewidth(0.5)\n",
    "    plt.show()\n",
    "    figname_file= os.path.join(figures_path, \n",
    "                               directory_name + '_' + filename + '{:%m%d%y}.png').format(today)\n",
    "    fig.savefig(figname_file, transparent= True)\n",
    "    #df_agging.plot.hist(bins=10, title= 'Aging tickets')\n",
    "    print(df_agging.describe())\n",
    "    df_aggingsum= df[(df.x_agingdays > 0) & (df.Type.isin(filter_in))]\n",
    "    aggingsum= df_aggingsum.groupby(['x_vendor', \n",
    "                                     'Type']).agg({'x_aging': 'sum',\n",
    "                                                   'number':'count', \n",
    "                                                   'x_agingdays':['mean',\n",
    "                                                                  'std','median']}).sort_values('x_vendor', \n",
    "                                                                                                ascending=False)\n",
    "\n",
    "    aggingsum.rename(columns = {'sum':'Open','count':'Closed', \n",
    "                                'std': 'Std Dev', \n",
    "                                'mean':'Mean', 'number':'','x_aging':'', 'x_agingdays':''}, inplace= True)\n",
    "    print(aggingsum)\n",
    "    \n",
    "def group_by(df):\n",
    "    ''' group by df to report in xls file\n",
    "    '''\n",
    "    #groub by 'yearmonth', 'dayweek', 'hourday', 'cmdb_ci_name','PandL'\n",
    "    grouped= df.groupby(['x_yearmonth', 'x_dayweek', 'x_hourday', 'cmdb_ci_name',\n",
    "                                'PandL'], \n",
    "                                as_index = False).agg({'closed_by_user_name' :['count', 'nunique'],\n",
    "                                                       'number' : 'count'})\n",
    "    grouped.columns = [\"_\".join(x) for x in grouped.columns.ravel()]\n",
    "    \n",
    "    #groub by 'yearmonth', 'cmdb_ci_name', 'PandL'\n",
    "    grouped1= df.groupby(['x_yearmonth', 'cmdb_ci_name', 'PandL'], \n",
    "                                 as_index = False).agg({'closed_by_user_name' :['count', 'nunique'],\n",
    "                                                        'number' : 'count'})\n",
    "    grouped1.columns = [\"_\".join(x) for x in grouped1.columns.ravel()]\n",
    "\n",
    "    #groub by file 'yearmonth', 'PandL'\n",
    "    grouped2= df.groupby(['x_yearmonth', 'PandL'], as_index = False).agg({'number' : 'count'})\n",
    "    return (grouped, grouped1, grouped2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "originalpath = (os.getcwd())\n",
    "print(originalpath)\n",
    "os.chdir(originalpath)\n",
    "#os.chdir('..')\n",
    "path = os.getcwd()\n",
    "print(path)\n",
    "\n",
    "today = datetime.datetime.today()\n",
    "directory_name= '{{cookiecutter.directory_name}}'\n",
    "report_file= os.path.join(path, 'reports',directory_name + '_report{:%m%d%y}.xlsx').format(today)\n",
    "figures_path= os.path.join(path, 'reports','figures')\n",
    "\n",
    "datefile= input('Date of file (MMDDYY: ')\n",
    "fileoriginaltickets = os.path.join(path, 'data','processed', directory_name + '_tickets' + datefile + '.pkl')\n",
    "fileoriginalapps = os.path.join(path, 'data','processed', directory_name + '_apps' + datefile + '.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read pkl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read PKL files\n",
    "df2 = pd.read_pickle(fileoriginaltickets,'gzip')\n",
    "dfreadfile = df2.copy()\n",
    "\n",
    "df3 = pd.read_pickle(fileoriginalapps,'gzip')\n",
    "dfreadfileapps = df3.copy()\n",
    "print('tickets: {}'.format(len(dfreadfile)))\n",
    "print('Apps: {}'.format(len(dfreadfileapps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group dataset tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped, grouped1, grouped2 = group_by(dfreadfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 80/20 analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get 80/20 table based in threshold could be cum_pc or AVG3M\n",
    "threshold = int(input(\"Enter threshold : [80]\") or '81')\n",
    "basedin = input('Based analysis in [cummulative pc] or avg last 3 months :') or ('cum_pc')\n",
    "crostab= pareto(dfreadfile,'closed_by_name', 'x_yearmonth', 'All','201812', '201811','201810',threshold)\n",
    "crostab[crostab[basedin] < threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution in the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution in the day\n",
    "gethrdistribution(dfreadfile, 'x_bins_day', 'number', 'Distribution in a day',0,'Distday')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution by type of tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#types of tickets\n",
    "gethrdistribution(dfreadfile, 'Type', 'number', 'Types of tickets',10, 'typetks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar chart tickets per vendor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barchart(dfreadfile,'number','x_vendor','Total Tickets', 'Tickets', 'Organization', 'org_tkts_bch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aging analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_in= ['Incident','Requested Item','Change']\n",
    "histogram(dfreadfile, 'x_agingdays', 'Agging Tickets', 'Aging in Days', 'Tickets', filter_in,  'agingtkts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Productivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#productivity\n",
    "print('Productivity= rate of output (tickets) per unit of input (hrs FTE)')\n",
    "sumprod= dfreadfile.groupby('x_vendor').agg({'number':'count',\n",
    "                                                       'closed_by_name':'nunique'}).sort_values('number', \n",
    "                                                                                        ascending=False)\n",
    "sumprod['Productivity']= sumprod['number'] / (sumprod['closed_by_name'] * 2000)\n",
    "sumprod['Tickets_per_month']= sumprod['number'] / 12 / sumprod['closed_by_name']\n",
    "#sumnuc1['Productivity vs effort']= sumnuc1['number'] / sumnuc1['cycletime'] \n",
    "sumprod.rename(columns = {'closed_by_name':'Unique Solvers','number':'Tickets'}, inplace= True)\n",
    "sumprod = sumprod[sumprod[\"Tickets\"] > 0]\n",
    "sumprod.index.rename('Org', inplace= True)\n",
    "sumprod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Excel file into reports directory\n",
    "\n",
    "Save an Excel file with intermediate results into the report directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = ExcelWriter(report_file,options={'strings_to_urls': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfreadfile.to_excel(writer, sheet_name='Tickets')\n",
    "grouped.to_excel(writer, sheet_name='G_by_day_hr_CI')\n",
    "grouped1.to_excel(writer, sheet_name='G_by_month_CI')\n",
    "grouped2.to_excel(writer, sheet_name='G_by_month_PL')\n",
    "dfreadfileapps.to_excel(writer, sheet_name= 'apps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
