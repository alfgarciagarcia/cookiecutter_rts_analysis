{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## {{cookiecutter.project_name}}\n",
    "\n",
    "{{cookiecutter.description}}\n",
    "\n",
    "This notebook contains basic statistical analysis and visualization of the data.\n",
    "\n",
    "### Data Sources\n",
    "- summary : Processed file from notebook 1-Data_Prep\n",
    "\n",
    "### Changes\n",
    "- {% now 'utc', '%m-%d-%Y' %} : Started project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import log\n",
    "from numpy.random import randn\n",
    "import glob\n",
    "import datetime as dt\n",
    "import pickle\n",
    "import os\n",
    "from pandas import ExcelWriter\n",
    "import re\n",
    "from zipfile import ZipFile\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import normaltest\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import anderson\n",
    "from scipy.stats import boxcox\n",
    "from scipy.stats import norm\n",
    "from math import erf, sqrt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#80/20 analysis\n",
    "def pareto(df, rows, columns, sortcol, colmonth1, colmonth2, colmonth3, minvalue):\n",
    "    '''df= dataframe to use\n",
    "    rows= column to use for rows\n",
    "    columns= name of column to use as rows\n",
    "    sortcol = column to sumarrize ej: tickets, cases, persons, etc)\n",
    "    colmonth1:colmonth3 = columns to calculate average for columns\n",
    "    minvalue = value to filter the result, will show recrods with values greater than minvalue\n",
    "    '''\n",
    "    crostab= pd.crosstab(df[rows],df[columns],margins=True)\n",
    "    crostab.sort_values(sortcol, ascending=False,inplace=True)\n",
    "    crostab= crostab.drop(['All'])\n",
    "    print('Total of rows: {}'.format(len(crostab)))\n",
    "    crostab['pc']= 100*crostab[sortcol]/crostab[sortcol].sum()\n",
    "    crostab['cum_pc']=crostab['pc'].cumsum()\n",
    "    crostab['AVG3M']= (crostab[colmonth1] + crostab[colmonth2] + crostab[colmonth3]) /3\n",
    "    print('Total of rows up to 80%: {}'.format(len(crostab[crostab['cum_pc'] <  81])))\n",
    "    print('{} Total of rows below average of {}'.format(len(crostab[crostab['AVG3M'] <=  minvalue]), minvalue))\n",
    "    print('to print the table run: crostab2[crostab2[\"AVG3M\"] > 5]')\n",
    "    return crostab\n",
    "\n",
    "#distribution\n",
    "def gethrdistribution(df, group1, agg1, titletxt= 'Pie Chart', minpercent=5, filename='figpie.png'):\n",
    "    '''pie distributions per group\n",
    "    consolidate % < 10% in others category    \n",
    "    '''\n",
    "    dist1= df.groupby(group1,as_index=False)[agg1].count()\n",
    "    dist1['pc']= 100*dist1[agg1]/dist1[agg1].sum()\n",
    "    dist1[group1]= np.where(dist1['pc']<minpercent,'Others',dist1[group1])\n",
    "    dist1= dist1.groupby(group1,as_index=False)[agg1].sum()\n",
    "    dist1['pc']= 100*dist1[agg1]/dist1[agg1].sum()\n",
    "    dist1= dist1.sort_values('pc', ascending=False)\n",
    "    dist1.reindex(copy=False)\n",
    "    dist1['cum_pc']=dist1['pc'].cumsum()\n",
    "    # Create a list of colors (from iWantHue)\n",
    "    colors = [ '#959a3c', '#55ac69', '#5b86d0', \"#E13F29\", \"#D69A80\", \"#D63B59\", \n",
    "              \"#AE5552\", \"#CB5C3B\", \"#EB8076\", \"#96624E\" ]\n",
    "    # Create a pie chart\n",
    "    fig, ax = plt.subplots()  \n",
    "    plt.pie(\n",
    "        dist1[agg1],         # using data agg1\n",
    "        labels=dist1[group1],# with the labels being group1\n",
    "        shadow=False, # with no shadows\n",
    "        colors=colors, # with colors\n",
    "        #explode=(0, 0.15, 0), # with one slide exploded out\n",
    "        # with the start angle at 90%\n",
    "        startangle=90, # with the start angle at 90%\n",
    "        autopct='%1.1f%%', # with the percent listed as a fraction \n",
    "        counterclock= False\n",
    "        )\n",
    "    # View the plot drop above\n",
    "    plt.axis('equal')\n",
    "    plt.title(titletxt)\n",
    "    # View the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    figname_file= os.path.join(figures_path,\n",
    "                               directory_name + '_' + filename + '{:%m%d%y}.png').format(today)\n",
    "    fig.savefig(figname_file, transparent= True)\n",
    "    dist1= dist1.sort_values('pc', ascending=False)\n",
    "    print(dist1)\n",
    "\n",
    "def plottickets(df, group1, group2, countfield):\n",
    "    '''plot df grouped by group1 and group2 and counting countfield'''\n",
    "    ts=df.groupby([group1,group2]).agg({countfield: 'count'})\n",
    "    #ts.sort_values(group1, ascending=True,inplace=True)\n",
    "    ts.plot(kind= 'line')\n",
    "    return ts\n",
    "\n",
    "def weedaysbars(df, group1, agg1, title, xlab, ylab, filename='figbarcharth.png'):\n",
    "    '''function to display bar chart, ej criticality, or weekdays barcharts'''\n",
    "    weekdays= df.groupby(group1,as_index=False)[agg1].count()\n",
    "    fig, ax = plt.subplots()  \n",
    "    #plt.bar(weekdays[group1], height= weekdays[agg1], color='#607c8e')\n",
    "    ax.bar(weekdays[group1], height= weekdays[agg1], color='#607c8e')\n",
    "    \n",
    "    width = 0.75 # the width of the bars \n",
    "    ax.barh(df.index, df['number'], width)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    ###\n",
    "    #for i, v in enumerate(weekdays[group1]):\n",
    "    #    ax.text(v + 3, i + .0, str(v))\n",
    "    ax.patch.set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_linewidth(0.5)\n",
    "    ax.spines['left'].set_linewidth(0.5)\n",
    "    ###\n",
    "    plt.show()\n",
    "    figname_file= os.path.join(figures_path, \n",
    "                               directory_name + '_' + filename + '{:%m%d%y}.png').format(today)\n",
    "    fig.savefig(figname_file, transparent= True)\n",
    "    print(weekdays)\n",
    "    \n",
    "def weedaysbarsh(df, group1, agg1, title, xlab, ylab, filename='figbarcharth.png'):\n",
    "    '''function to display bar chart, ej criticality, or weekdays barcharts'''\n",
    "    weekdays= df.groupby(group1,as_index=False)[agg1].count()\n",
    "    fig, ax = plt.subplots()  \n",
    "    \n",
    "    width = 0.75 # the width of the bars \n",
    "    ax.barh(weekdays[group1], weekdays[agg1], width)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    for i, v in enumerate(weekdays[agg1]):\n",
    "        ax.text(v + 3, i + .0, str(v))\n",
    "    ax.patch.set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_linewidth(0.5)\n",
    "    ax.spines['left'].set_linewidth(0.5)\n",
    "    plt.show()\n",
    "    figname_file= os.path.join(figures_path, \n",
    "                               directory_name + '_' + filename + '{:%m%d%y}.png').format(today)\n",
    "    fig.savefig(figname_file, transparent= True)\n",
    "    print(weekdays)   \n",
    "    \n",
    "#cycle_time3\n",
    "def cycletime3(df, groupby2, groupby3, agg1, agg2, agg3):\n",
    "    '''Caclulate cycletime per vendor just for request and incidents\n",
    "    usage: cycletime2(nuclear0,'yearmonth','Vendor_Closeassign','cycletime','number',\n",
    "                    'closed_by_user_name', 'cycletime')\n",
    "    '''\n",
    "    df = df[df.Type.isin(['Requested Item','Incident'])]\n",
    "    #cycle_time and FTE\n",
    "    df2= df.groupby([groupby2,groupby3]).agg({agg1: ['mean','std','max','min'], \n",
    "                                                 agg2: 'count',agg3: 'nunique'})\n",
    "    # Using ravel, and a string join, we can create better names for the columns:\n",
    "    df2.columns = [\"_\".join(x) for x in df2.columns.ravel()]\n",
    "    agg5= agg3 + '_nunique'\n",
    "    agg6= agg2 + '_count'\n",
    "    agg7= agg1 + '_mean'\n",
    "    # per month\n",
    "    df2= df2.groupby([groupby3]).agg({agg5: ['mean', 'std'], agg6: ['mean','count', 'median','max'], \n",
    "                                      agg7: ['mean','std', 'median']})\n",
    "    return df2\n",
    "\n",
    "def barchart(df,x,y,title, x_label, y_label,filename='figbarchart.png'):\n",
    "    '''bar chart tickets per organizatio x_Vendor_Closeassign or vendor'''\n",
    "    field_vendor = 'x_Vendor_Closeassign'\n",
    "    field_vendor = 'x_vendor'\n",
    "    pt_df= df.pivot_table(x, index=[y],\n",
    "                                    aggfunc='count',\n",
    "                                    margins=True)#.sort_values(('SSO','All'), ascending=False)\n",
    "    pt_df.index.rename(y_label, inplace= True)\n",
    "    #remove rows with cero count of tickets\n",
    "    pt_df= pt_df[pt_df[x] >0].sort_values(x, ascending=True)\n",
    "\n",
    "    fig, ax = plt.subplots()    \n",
    "    width = 0.75 # the width of the bars \n",
    "    ax.barh(pt_df.index, pt_df[x], width)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)   \n",
    "    for i, v in enumerate(pt_df[x]):\n",
    "        ax.text(v + 3, i + .0, str(v))\n",
    "    ax.patch.set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_linewidth(0.5)\n",
    "    ax.spines['left'].set_linewidth(0.5)\n",
    "    plt.show()\n",
    "    figname_file= os.path.join(figures_path, \n",
    "                               directory_name + '_' + filename + '{:%m%d%y}.png').format(today)\n",
    "    fig.savefig(figname_file, transparent= True)\n",
    "\n",
    "def histogram(df,x, title, x_label, y_label, filter_in, filename= 'histogram'):\n",
    "    #histogram aging tickets \n",
    "    df_agging=df[(df.x_agingdays > 0) &(df.Type.isin(filter_in))]\n",
    "    df_agging= df_agging[x]\n",
    "    fig, ax = plt.subplots() \n",
    "    ax.hist(df_agging, bins=10)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label) \n",
    "    ax.patch.set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_linewidth(0.5)\n",
    "    ax.spines['left'].set_linewidth(0.5)\n",
    "    plt.show()\n",
    "    figname_file= os.path.join(figures_path, \n",
    "                               directory_name + '_' + filename + '{:%m%d%y}.png').format(today)\n",
    "    fig.savefig(figname_file, transparent= True)\n",
    "    #df_agging.plot.hist(bins=10, title= 'Aging tickets')\n",
    "    print(df_agging.describe())\n",
    "    df_aggingsum= df[(df.x_agingdays > 0) & (df.Type.isin(filter_in))]\n",
    "    aggingsum= df_aggingsum.groupby(['x_vendor', \n",
    "                                     'Type']).agg({'x_aging': 'sum',\n",
    "                                                   'number':'count', \n",
    "                                                   'x_agingdays':['mean',\n",
    "                                                                  'std','median']}).sort_values('x_vendor', \n",
    "                                                                                                ascending=False)\n",
    "\n",
    "    aggingsum.rename(columns = {'sum':'Open','count':'Closed', \n",
    "                                'std': 'Std Dev', \n",
    "                                'mean':'Mean', 'number':'','x_aging':'', 'x_agingdays':''}, inplace= True)\n",
    "    print(aggingsum)\n",
    "    \n",
    "def group_by(df):\n",
    "    ''' group by df to report in xls file\n",
    "    '''\n",
    "    #groub by 'yearmonth', 'dayweek', 'hourday', 'cmdb_ci_name','PandL'\n",
    "    grouped= df.groupby(['x_yearmonth', 'x_dayweek', 'x_hourday', 'cmdb_ci_name',\n",
    "                                'PandL'], \n",
    "                                as_index = False).agg({'closed_by_user_name' :['count', 'nunique'],\n",
    "                                                       'number' : 'count'})\n",
    "    grouped.columns = [\"_\".join(x) for x in grouped.columns.ravel()]\n",
    "    \n",
    "    #groub by 'yearmonth', 'cmdb_ci_name', 'PandL'\n",
    "    grouped1= df.groupby(['x_yearmonth', 'cmdb_ci_name', 'PandL'], \n",
    "                                 as_index = False).agg({'closed_by_user_name' :['count', 'nunique'],\n",
    "                                                        'number' : 'count'})\n",
    "    grouped1.columns = [\"_\".join(x) for x in grouped1.columns.ravel()]\n",
    "\n",
    "    #groub by file 'yearmonth', 'PandL'\n",
    "    grouped2= df.groupby(['x_yearmonth', 'PandL'], as_index = False).agg({'number' : 'count'})\n",
    "    return (grouped, grouped1, grouped2)\n",
    "\n",
    "def verify_normality(df, column):\n",
    "    ''' graph distribution for a column, with values > 0\n",
    "    '''\n",
    "    print(df[column].describe())\n",
    "    df2= df[df[column] > 0]\n",
    "    arr = df2[column]\n",
    "    mean=arr.mean()\n",
    "    median=arr.median()\n",
    "    mode=arr.mode()\n",
    "    print('Mean: ',mean,'\\nMedian: ',median,'\\nMode: ',mode[0])\n",
    "\n",
    "    arr = sorted(arr)\n",
    "    fit = stats.norm.pdf(arr, np.mean(arr), np.std(arr)) \n",
    " \n",
    "    #plot both series on the histogram\n",
    "    fig, ax = plt.subplots() \n",
    "    plt.axvline(mean,color='red',label='Mean')\n",
    "    plt.axvline(median,color='yellow',label='Median')\n",
    "    plt.axvline(mode[0],color='green',label='Mode')\n",
    "    plt.plot(arr,fit,'-',linewidth = 2,label=\"Normal distribution with same mean and var\")\n",
    "    plt.hist(arr,density=True,bins = 10,label=\"Actual distribution\")   \n",
    "    ax.patch.set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_linewidth(0.5)\n",
    "    ax.spines['left'].set_linewidth(0.5)\n",
    "    #plt.title('Histogram {}'.format(column))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(211)\n",
    "    prob = stats.probplot(df2[column], dist=stats.norm, plot=ax1)\n",
    "    ax1.set_xlabel('')\n",
    "    ax1.set_title('Probplot against normal distribution')\n",
    "    \n",
    "def transform(df, column, method='power'):    \n",
    "    '''Transform a column using log,scale, minmax, boxcox,  power, norm\n",
    "    filter out rows with values <=0, takes only positive values.\n",
    "    '''\n",
    "    dfnorm= pd.DataFrame()\n",
    "    df= df[df[column] > 0]\n",
    "    df[column]= df[column].fillna(df[column].mean())\n",
    "    dfnorm['x_original']= df[column]  \n",
    "    print(df[column].describe())\n",
    "    x_array = np.array(df[[column]])\n",
    "    if method== 'norm':   #Scale transformation\n",
    "        x_scaled = preprocessing.normalize(x_array, norm= 'l2')\n",
    "        dfnorm['x_transformed'] = pd.DataFrame(x_scaled)\n",
    "    if method== 'log':    #Log transformation'\n",
    "        dfnorm['x_transformed'] = log(df[column])\n",
    "        #plt.hist(dfnorm['log'])\n",
    "    if method== 'sqt':    #Square root transformation\n",
    "        dfnorm['x_transformed'] = np.square(df[column])\n",
    "    if method== 'boxcox': #Box Cox transformatio\n",
    "        xt = stats.boxcox(df[column], lmbda=0)\n",
    "        dfnorm['x_transformed']= xt\n",
    "    if method== 'minmax': #minmax transformation\n",
    "        # Create a minimum and maximum processor object\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        # Create an object to transform the data to fit minmax processor\n",
    "        x_scaled = min_max_scaler.fit_transform(x_array)\n",
    "        # Run the normalizer on the dataframe\n",
    "        dfnorm['x_transformed'] = pd.DataFrame(x_scaled)\n",
    "        dfnorm['x_transformed']= dfnorm['x_transformed'].fillna(dfnorm['x_transformed'].mean())\n",
    "    if method== 'power' :\n",
    "        pt= preprocessing.PowerTransformer(method= 'box-cox',standardize=False)\n",
    "        dfnorm['x_transformed']= pt.fit_transform(x_array)\n",
    "    if method== 'scale':\n",
    "        x_scaled = preprocessing.scale(x_array)\n",
    "        dfnorm['x_transformed'] = pd.DataFrame(x_scaled)\n",
    "    print(dfnorm['x_transformed'].describe())\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(321)\n",
    "    ax2 = fig.add_subplot(322)\n",
    "    ax3 = fig.add_subplot(323)\n",
    "    ax4 = fig.add_subplot(324) \n",
    "    ax1.hist(dfnorm['x_original'])\n",
    "    ax1.set_title= ('Histogram before {} transformation for {}'.format(method, column))\n",
    "    ax2.hist(dfnorm['x_transformed'])\n",
    "    ax2.set_title= ('Histogram after {} transformation for {}'.format(method, column))\n",
    "    prob2 = stats.probplot(dfnorm['x_transformed'], dist=stats.norm, plot=ax3)\n",
    "    ax3.set_title('Probplot after {} transformation'.format(method))\n",
    "    ax4.set_title('BoxPlot')\n",
    "    red_square = dict(markerfacecolor='r', marker='s')\n",
    "    ax4.boxplot(dfnorm['x_transformed'], vert=False, flierprops=red_square)\n",
    "    plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.6, wspace=0.35)\n",
    "    plt.show()\n",
    "    return dfnorm\n",
    "\n",
    "def nomality_tests(df, column, alpha= 0.05):\n",
    "    '''Test normality using D'Angostino & Pearson, Sahpiro, Anderson-Darling\n",
    "    '''\n",
    "    x= df[column]\n",
    "    stat, p = normaltest(x)  #D'Angostino & Pearson test\n",
    "    print(' D Angostino = {:.3f} pvalue = {:.4f}'.format(stat, p))\n",
    "    if p > alpha:\n",
    "        print('   data looks normal (fail to reject H0)')\n",
    "    else:\n",
    "        print('   data does not look normal (reject H0)')\n",
    "    if len(x) < 5000:  #Shapiro test is reliable with less than 5K records\n",
    "        stat, p = shapiro(x)\n",
    "        print(' Shapiro = {:.3f} pvalue = {:.4f}'.format(stat, p))\n",
    "        if p > alpha:\n",
    "            print('   data looks normal (fail to reject H0)')\n",
    "        else:\n",
    "            print('   data does not look normal (reject H0)')\n",
    "    stat = anderson(x, dist='norm')\n",
    "    print(' Anderson = {:.3f}  '.format(stat.statistic))\n",
    "    for i in range(len(stat.critical_values)):\n",
    "        sl, cv = stat.significance_level[i], stat.critical_values[i]\n",
    "        if stat.statistic < stat.critical_values[i]:\n",
    "            print('   {:.3f}: {:.3f}, data looks normal (fail to reject H0)'.format(sl, cv))\n",
    "        else:\n",
    "            print('   {:.3f}: {:.3f}, data does not look normal (reject H0)'.format(sl, cv))\n",
    "        print('   SL: {} cv = {}'.format(sl, cv))\n",
    "\n",
    "\n",
    "def outliers_iqr(df, column, output= 'x_outlier'):\n",
    "    '''Interquartile range method to detect outliers\n",
    "    return a df with column for outlier default name x_outlier\n",
    "    '''\n",
    "    quartile_1, quartile_3 = np.percentile(df[column], [25, 75])\n",
    "    iqr = quartile_3 - quartile_1\n",
    "    lower_bound = quartile_1 - (iqr * 1.5)\n",
    "    upper_bound = quartile_3 + (iqr * 1.5)\n",
    "    df[output] = np.where((df[column] > upper_bound) | (df[column] < lower_bound),1,0)\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(321)\n",
    "    ax2 = fig.add_subplot(322)\n",
    "    red_square = dict(markerfacecolor='r', marker='s')\n",
    "    ax1.boxplot(df[column], vert=False, flierprops=red_square)\n",
    "    ax1.set_title('{} Before'.format(column))\n",
    "    ax2.boxplot(df[output], vert=False, flierprops=red_square)\n",
    "    ax2.set_title('{} After'.format(column))\n",
    "    plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.6, wspace=0.35)\n",
    "    plt.show()\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "originalpath = (os.getcwd())\n",
    "print(originalpath)\n",
    "os.chdir(originalpath)\n",
    "#os.chdir('..')\n",
    "path = os.getcwd()\n",
    "print(path)\n",
    "\n",
    "today = datetime.datetime.today()\n",
    "directory_name= '{{cookiecutter.directory_name}}'\n",
    "report_file= os.path.join(path, 'reports',directory_name + '_report{:%m%d%y}.xlsx').format(today)\n",
    "figures_path= os.path.join(path, 'reports','figures')\n",
    "\n",
    "datefile= input('Date of file (MMDDYY: ')\n",
    "fileoriginaltickets = os.path.join(path, 'data','processed', directory_name + '_tickets' + datefile + '.pkl')\n",
    "fileoriginalapps = os.path.join(path, 'data','processed', directory_name + '_apps' + datefile + '.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read pkl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read PKL files\n",
    "df2 = pd.read_pickle(fileoriginaltickets,'gzip')\n",
    "dfreadfile = df2.copy()\n",
    "\n",
    "df3 = pd.read_pickle(fileoriginalapps,'gzip')\n",
    "dfreadfileapps = df3.copy()\n",
    "print('tickets: {}'.format(len(dfreadfile)))\n",
    "print('Apps: {}'.format(len(dfreadfileapps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group dataset tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped, grouped1, grouped2 = group_by(dfreadfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 80/20 analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get 80/20 table based in threshold could be cum_pc or AVG3M\n",
    "threshold = int(input(\"Enter threshold : [80]\") or '81')\n",
    "basedin = input('Based analysis in [cum_pc] or avg last 3 months [AVG3M] :') or ('cum_pc')\n",
    "column= input('Column to use [cmdb_ci_name]: ') or ('cmdb_ci_name')\n",
    "crostab= pareto(dfreadfile, column, 'x_yearmonth', 'All','201812', '201811','201810',threshold)\n",
    "crostab[crostab[basedin] < threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pareto graph\n",
    "ct= crostab[crostab[basedin] < threshold]\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(ct.index, ct.All, color=\"C0\")\n",
    "plt.xticks(ct.index, rotation='vertical', size=6)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(ct.index, ct.cum_pc, color=\"C2\", marker=\",\", ms=5)\n",
    "ax2.yaxis.set_major_formatter(PercentFormatter())\n",
    "ax.set_title('Pareto {}'.format(column))\n",
    "ax.tick_params(axis=\"y\", colors=\"C0\")\n",
    "ax2.tick_params(axis=\"y\", colors=\"C2\")\n",
    "#plt.xticks(ct.index, rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution in the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution in the day\n",
    "gethrdistribution(dfreadfile, 'x_bins_day', 'number', 'Distribution in a day',0,'Distday')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution by type of tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#types of tickets\n",
    "gethrdistribution(dfreadfile, 'Type', 'number', 'Types of tickets',10, 'typetks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar chart tickets per vendor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barchart(dfreadfile,'number','x_vendor','Total Tickets', 'Tickets', 'Organization', 'org_tkts_bch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aging analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_in= ['Incident','Requested Item','Change']\n",
    "histogram(dfreadfile, 'x_agingdays', 'Agging Tickets', 'Aging in Days', 'Tickets', filter_in,  'agingtkts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Productivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#productivity\n",
    "print('Productivity= rate of output (tickets) per unit of input (hrs FTE)')\n",
    "sumprod= dfreadfile.groupby('x_vendor').agg({'number':'count',\n",
    "                                                       'closed_by_name':'nunique'}).sort_values('number', \n",
    "                                                                                        ascending=False)\n",
    "sumprod['Productivity']= sumprod['number'] / (sumprod['closed_by_name'] * 2000)\n",
    "sumprod['Tickets_per_month']= sumprod['number'] / 12 / sumprod['closed_by_name']\n",
    "#sumnuc1['Productivity vs effort']= sumnuc1['number'] / sumnuc1['cycletime'] \n",
    "sumprod.rename(columns = {'closed_by_name':'Unique Solvers','number':'Tickets'}, inplace= True)\n",
    "sumprod = sumprod[sumprod[\"Tickets\"] > 0]\n",
    "sumprod.index.rename('Org', inplace= True)\n",
    "sumprod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type distribution continues variables (cycletime, agging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_normality(dfreadfile, 'x_cycletime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_normality(dfreadfile, 'x_agingdays')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normality Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomality_tests(dfreadfile, 'x_cycletime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform continues variables  (cycletime, agging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrans= transform(dfreadfile, 'x_cycletime','power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomality_tests(dftrans, 'x_transformed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if not transformed, run outliers over original df\n",
    "dftrans=  outliers_iqr(dftrans, 'x_transformed')\n",
    "print('outliers {}'.format(dftrans.x_outlier.sum()))\n",
    "\n",
    "dftrans=  outliers_iqr(dftrans, 'x_original', 'x_outlier2')\n",
    "print('outliers2 {}'.format(dftrans.x_outlier2.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge outliers in original df, if transformed/normalized\n",
    "dfreadfile= pd.merge(dfreadfile, dftrans[['x_outlier']], right_index=True, left_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area under the curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = dfreadfile.x_cycletime.mean()\n",
    "sigma = dfreadfile.x_cycletime.std()\n",
    "x1 = .25  #lower limit 2 hrs\n",
    "x2 = 8   #upper limit 4 days\n",
    "# calculate probability\n",
    "# probability from Z=0 to lower bound\n",
    "double_prob = erf( (x1-mu) / (sigma*sqrt(2)) )\n",
    "p_lower = double_prob/2\n",
    "print('\\n Lower Bound: {}'.format(round(p_lower,4)))\n",
    "\n",
    "# probability from Z=0 to upper bound\n",
    "double_prob = erf( (x2-mu) / (sigma*sqrt(2)) )\n",
    "p_upper = double_prob/2\n",
    "print(' Upper Bound: {}'.format(round(p_upper,4)))\n",
    "\n",
    "# print the results\n",
    "Pin = (p_upper) - (p_lower)\n",
    "print('\\n')\n",
    "print('mean = {}    std dev = {} \\n'.format(mu, sigma))\n",
    "print('Calculating the probability of occurring between {} <--> {} days\\n'.format(x1, x2))\n",
    "print('inside interval Pin = {}%'.format(round(Pin*100,1)))\n",
    "print('outside interval Pout = {}% \\n'.format(round((1-Pin)*100,1)))\n",
    "print('\\n')\n",
    "\n",
    "# calculate the z-transform\n",
    "z1 = ( x1 - mu ) / sigma\n",
    "z2 = ( x2 - mu ) / sigma\n",
    "\n",
    "x = np.arange(z1, z2, 0.001) # range of x in spec\n",
    "x_all = np.arange(-10, 10, 0.001) # entire range of x, both in and out of spec\n",
    "# mean = 0, stddev = 1, since Z-transform was calculated\n",
    "y = norm.pdf(x,0,1)\n",
    "y2 = norm.pdf(x_all,0,1)\n",
    "\n",
    "# build the plot\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "plt.style.use('fivethirtyeight')\n",
    "ax.plot(x_all,y2)\n",
    "\n",
    "ax.fill_between(x,y,0, alpha=0.3, color='b')\n",
    "ax.fill_between(x_all,y2,0, alpha=0.1)\n",
    "ax.set_xlim([-4,4])\n",
    "ax.set_xlabel('# of Standard Deviations Outside the Mean')\n",
    "ax.set_yticklabels([])\n",
    "ax.set_title('Probability to comply')\n",
    "\n",
    "plt.savefig('normal_curve.png', dpi=72, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Excel file into reports directory\n",
    "\n",
    "Save an Excel file with intermediate results into the report directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = ExcelWriter(report_file,options={'strings_to_urls': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfreadfile.to_excel(writer, sheet_name='Tickets')\n",
    "grouped.to_excel(writer, sheet_name='G_by_day_hr_CI')\n",
    "grouped1.to_excel(writer, sheet_name='G_by_month_CI')\n",
    "grouped2.to_excel(writer, sheet_name='G_by_month_PL')\n",
    "dfreadfileapps.to_excel(writer, sheet_name= 'apps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
